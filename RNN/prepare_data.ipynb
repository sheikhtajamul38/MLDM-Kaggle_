{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 1854.752652,
      "end_time": "2023-10-13T02:24:31.866910",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2023-10-13T01:53:37.114258",
      "version": "2.4.0"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30626,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Finding Critical Points using RNNs\n",
        "\n",
        "Credits:\n",
        " - dataloader: https://www.kaggle.com/code/henriupton/efficient-loading-memory-usage-visualizations-cmi"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.002856,
          "end_time": "2023-10-13T01:53:40.395162",
          "exception": false,
          "start_time": "2023-10-13T01:53:40.392306",
          "status": "completed"
        },
        "tags": [],
        "id": "5gsUb_Q_FnEM"
      },
      "id": "5gsUb_Q_FnEM"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gc\n",
        "import time\n",
        "import json\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import joblib\n",
        "import random\n",
        "import math\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from scipy.interpolate import interp1d\n",
        "\n",
        "from math import pi, sqrt, exp\n",
        "import sklearn,sklearn.model_selection\n",
        "import torch\n",
        "from torch import nn,Tensor\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\n",
        "from sklearn.metrics import average_precision_score\n",
        "from timm.scheduler import CosineLRScheduler\n",
        "plt.style.use(\"ggplot\")\n",
        "\n",
        "from pyarrow.parquet import ParquetFile\n",
        "import pyarrow as pa\n",
        "import ctypes"
      ],
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2023-10-13T01:53:40.404034Z",
          "iopub.status.busy": "2023-10-13T01:53:40.403399Z",
          "iopub.status.idle": "2023-10-13T01:53:48.270243Z",
          "shell.execute_reply": "2023-10-13T01:53:48.269439Z"
        },
        "papermill": {
          "duration": 7.874869,
          "end_time": "2023-10-13T01:53:48.272549",
          "exception": false,
          "start_time": "2023-10-13T01:53:40.397680",
          "status": "completed"
        },
        "tags": [],
        "id": "MzF1LWAtFnEO"
      },
      "execution_count": null,
      "outputs": [],
      "id": "MzF1LWAtFnEO"
    },
    {
      "cell_type": "code",
      "source": [
        "class PATHS:\n",
        "    MAIN_DIR = \"/kaggle/input/child-mind-institute-detect-sleep-states/\"\n",
        "    # CSV FILES :\n",
        "    SUBMISSION = MAIN_DIR + \"sample_submission.csv\"\n",
        "    TRAIN_EVENTS = MAIN_DIR + \"train_events.csv\"\n",
        "    # PARQUET FILES:\n",
        "    TRAIN_SERIES = MAIN_DIR + \"train_series.parquet\"\n",
        "    TEST_SERIES = MAIN_DIR + \"test_series.parquet\""
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-13T01:53:48.279591Z",
          "iopub.status.busy": "2023-10-13T01:53:48.278968Z",
          "iopub.status.idle": "2023-10-13T01:53:48.283734Z",
          "shell.execute_reply": "2023-10-13T01:53:48.282737Z"
        },
        "papermill": {
          "duration": 0.010957,
          "end_time": "2023-10-13T01:53:48.286150",
          "exception": false,
          "start_time": "2023-10-13T01:53:48.275193",
          "status": "completed"
        },
        "tags": [],
        "id": "bQXBEUPsFnEP"
      },
      "execution_count": null,
      "outputs": [],
      "id": "bQXBEUPsFnEP"
    },
    {
      "cell_type": "code",
      "source": [
        "class CFG:\n",
        "    DEMO_MODE = True"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-13T01:53:48.292625Z",
          "iopub.status.busy": "2023-10-13T01:53:48.292251Z",
          "iopub.status.idle": "2023-10-13T01:53:48.297256Z",
          "shell.execute_reply": "2023-10-13T01:53:48.296432Z"
        },
        "papermill": {
          "duration": 0.010476,
          "end_time": "2023-10-13T01:53:48.299103",
          "exception": false,
          "start_time": "2023-10-13T01:53:48.288627",
          "status": "completed"
        },
        "tags": [],
        "id": "aSY5mCu2FnEP"
      },
      "execution_count": null,
      "outputs": [],
      "id": "aSY5mCu2FnEP"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class data_reader:\n",
        "    def __init__(self, demo_mode):\n",
        "        super().__init__()\n",
        "        # MAPPING FOR DATA LOADING :\n",
        "        self.names_mapping = {\n",
        "            \"submission\": {\"path\": PATHS.SUBMISSION, \"is_parquet\": False, \"has_timestamp\": False},\n",
        "            \"train_events\": {\"path\": PATHS.TRAIN_EVENTS, \"is_parquet\": False, \"has_timestamp\": True},\n",
        "            \"train_series\": {\"path\": PATHS.TRAIN_SERIES, \"is_parquet\": True, \"has_timestamp\": True},\n",
        "            \"test_series\": {\"path\": PATHS.TEST_SERIES, \"is_parquet\": True, \"has_timestamp\": True}\n",
        "        }\n",
        "        self.valid_names = [\"submission\", \"train_events\", \"train_series\", \"test_series\"]\n",
        "        self.demo_mode = demo_mode\n",
        "\n",
        "    def verify(self, data_name):\n",
        "        \"Function for data name verification\"\n",
        "        if data_name not in self.valid_names:\n",
        "            print(\"PLEASE ENTER A VALID DATASET NAME, VALID NAMES ARE: \", self.valid_names)\n",
        "        return\n",
        "\n",
        "    def cleaning(self, data):\n",
        "        \"Cleaning function: drop NA values\"\n",
        "        before_cleaning = len(data)\n",
        "        print(\"Number of missing timestamps: \", len(data[data[\"timestamp\"].isna()]))\n",
        "        data = data.dropna(subset=[\"timestamp\"])\n",
        "        after_cleaning = len(data)\n",
        "        print(\"Percentage of removed steps: {:.1f}%\".format(100 * (before_cleaning - after_cleaning) / before_cleaning))\n",
        "        return data\n",
        "\n",
        "    @staticmethod\n",
        "    def reduce_memory_usage(data):\n",
        "        \"Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.\"\n",
        "        start_mem = data.memory_usage().sum() / 1024 ** 2\n",
        "        print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "        for col in data.columns:\n",
        "            col_type = data[col].dtype\n",
        "            if col_type != object:\n",
        "                c_min = data[col].min()\n",
        "                c_max = data[col].max()\n",
        "                if str(col_type)[:3] == 'int':\n",
        "                    for int_type in [np.int8, np.int16, np.int32, np.int64]:\n",
        "                        if c_min > np.iinfo(int_type).min and c_max < np.iinfo(int_type).max:\n",
        "                            data[col] = data[col].astype(int_type)\n",
        "                            break\n",
        "                else:\n",
        "                    for float_type in [np.float16, np.float32, np.float64]:\n",
        "                        if c_min > np.finfo(float_type).min and c_max < np.finfo(float_type).max:\n",
        "                            data[col] = data[col].astype(float_type)\n",
        "                            break\n",
        "            else:\n",
        "                data[col] = data[col].astype('category')\n",
        "\n",
        "        end_mem = data.memory_usage().sum() / 1024 ** 2\n",
        "        print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "        print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "        return data\n",
        "\n",
        "    def load_data(self, data_name):\n",
        "        \"Function for data loading\"\n",
        "        self.verify(data_name)\n",
        "        data_props = self.names_mapping[data_name]\n",
        "        if data_props[\"is_parquet\"]:\n",
        "            if self.demo_mode:\n",
        "                pf = ParquetFile(data_props[\"path\"])\n",
        "                demo_steps = next(pf.iter_batches(batch_size=20_000))\n",
        "                data = pa.Table.from_batches([demo_steps]).to_pandas()\n",
        "            else:\n",
        "                data = pd.read_parquet(data_props[\"path\"])\n",
        "        else:\n",
        "            if self.demo_mode:\n",
        "                data = pd.read_csv(data_props[\"path\"], nrows=20_000)\n",
        "            else:\n",
        "                data = pd.read_csv(data_props[\"path\"])\n",
        "\n",
        "        gc.collect()\n",
        "        if data_props[\"has_timestamp\"]:\n",
        "            print('Cleaning')\n",
        "            data = self.cleaning(data)\n",
        "            gc.collect()\n",
        "        data = self.reduce_memory_usage(data)\n",
        "        return data\n"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-13T01:53:48.306173Z",
          "iopub.status.busy": "2023-10-13T01:53:48.305539Z",
          "iopub.status.idle": "2023-10-13T01:53:48.323582Z",
          "shell.execute_reply": "2023-10-13T01:53:48.322198Z"
        },
        "papermill": {
          "duration": 0.024686,
          "end_time": "2023-10-13T01:53:48.326279",
          "exception": false,
          "start_time": "2023-10-13T01:53:48.301593",
          "status": "completed"
        },
        "tags": [],
        "id": "AYOyWHNLFnEP"
      },
      "execution_count": null,
      "outputs": [],
      "id": "AYOyWHNLFnEP"
    },
    {
      "cell_type": "code",
      "source": [
        "reader = data_reader(demo_mode=False)\n",
        "series = reader.load_data(data_name=\"train_series\")\n",
        "events = reader.load_data(data_name=\"train_events\")"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-13T01:53:48.334186Z",
          "iopub.status.busy": "2023-10-13T01:53:48.333512Z",
          "iopub.status.idle": "2023-10-13T01:56:40.566680Z",
          "shell.execute_reply": "2023-10-13T01:56:40.565696Z"
        },
        "papermill": {
          "duration": 172.240081,
          "end_time": "2023-10-13T01:56:40.568947",
          "exception": false,
          "start_time": "2023-10-13T01:53:48.328866",
          "status": "completed"
        },
        "tags": [],
        "id": "ZtzFnNtLFnEQ",
        "outputId": "94881779-d9ed-4ab0-b391-1ecf81c597c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "cleaning\n\nNumber of missing timestamps :  0\n\nPercentage of removed steps : 0.0%\n\nMemory usage of dataframe is 3416.54 MB\n\nMemory usage after optimization is: 2059.05 MB\n\nDecreased by 39.7%\n\ncleaning\n\nNumber of missing timestamps :  4923\n\nPercentage of removed steps : 33.9%\n\nMemory usage of dataframe is 0.44 MB\n\nMemory usage after optimization is: 0.50 MB\n\nDecreased by -13.5%\n"
        }
      ],
      "id": "ZtzFnNtLFnEQ"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import joblib\n",
        "\n",
        "targets = []\n",
        "data = []\n",
        "ids = series.series_id.unique()\n",
        "\n",
        "for viz_id in tqdm(ids, desc=\"Processing Series IDs\"):\n",
        "    viz_targets = []\n",
        "    viz_events = events[events.series_id == viz_id]\n",
        "    viz_series = series.loc[series.series_id == viz_id].copy().reset_index()\n",
        "    viz_series['dt'] = pd.to_datetime(viz_series.timestamp, format='%Y-%m-%dT%H:%M:%S%z').astype(\"datetime64[ns, UTC-04:00]\")\n",
        "    viz_series['hour'] = viz_series['dt'].dt.hour\n",
        "\n",
        "    check = 0\n",
        "    for i in range(len(viz_events) - 1):\n",
        "        if (\n",
        "            viz_events.iloc[i].event == 'onset'\n",
        "            and viz_events.iloc[i + 1].event == 'wakeup'\n",
        "            and viz_events.iloc[i].night == viz_events.iloc[i + 1].night\n",
        "        ):\n",
        "            start, end = viz_events.timestamp.iloc[i], viz_events.timestamp.iloc[i + 1]\n",
        "\n",
        "            start_id = viz_series.loc[viz_series.timestamp == start].index.values[0]\n",
        "            end_id = viz_series.loc[viz_series.timestamp == end].index.values[0]\n",
        "            viz_targets.append((start_id, end_id))\n",
        "            check += 1\n",
        "\n",
        "    targets.append(viz_targets)\n",
        "    data.append(viz_series[['anglez', 'enmo', 'step']])  # Include additional features like 'hour', 'minute', or 'second' if needed\n",
        "\n",
        "joblib.dump((targets, data, ids), 'train_data.pkl')\n",
        "print(f\"Number of processed series: {len(data)}\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-13T01:56:40.576546Z",
          "iopub.status.busy": "2023-10-13T01:56:40.575812Z",
          "iopub.status.idle": "2023-10-13T02:24:29.731168Z",
          "shell.execute_reply": "2023-10-13T02:24:29.729802Z"
        },
        "papermill": {
          "duration": 1669.161337,
          "end_time": "2023-10-13T02:24:29.733162",
          "exception": false,
          "start_time": "2023-10-13T01:56:40.571825",
          "status": "completed"
        },
        "tags": [],
        "id": "HXAwxilPFnER",
        "outputId": "bc20ef62-9000-4dc8-8b39-048b38ae3bc8",
        "colab": {
          "referenced_widgets": [
            "cf91544647a5432a9a62fb3361ca008f"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cf91544647a5432a9a62fb3361ca008f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/277 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "execution_count": 6,
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "277"
            ]
          },
          "metadata": {}
        }
      ],
      "id": "HXAwxilPFnER"
    }
  ]
}